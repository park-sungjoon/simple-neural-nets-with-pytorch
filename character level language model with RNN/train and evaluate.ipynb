{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "import dset, model # by importing dset, we also import the text file and the related data.\n",
    "import numpy as np\n",
    "import datetime\n",
    "import utils\n",
    "char_size = dset.char_size\n",
    "from torch.utils.data import DataLoader # batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cuda.\n"
     ]
    }
   ],
   "source": [
    "MODEL_STR = \"LSTM\"\n",
    "train_size = 25 # number of characters used for single iteration (single computation of loss)\n",
    "h_size = 512 # size of hidden state\n",
    "depth = 1 # number of hidden layers in RNN\n",
    "batch_size = 512\n",
    "device = 'cuda'#torch.device(\"cuda\" if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "print(f\"training on {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model = getattr(model,MODEL_STR)\n",
    "rnn = rnn_model(char_size, \n",
    "                char_size, \n",
    "                h_size,\n",
    "                depth,\n",
    "                dropout = 0.5\n",
    "               ).to(device) # define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 1 out of 5000 // loss:  tensor(112.4639, device='cuda:0')\n",
      "Iter: 500 out of 5000 // loss:  tensor(37.8586, device='cuda:0')\n",
      "Iter: 1000 out of 5000 // loss:  tensor(32.1557, device='cuda:0')\n",
      "Iter: 1500 out of 5000 // loss:  tensor(30.9986, device='cuda:0')\n",
      "Iter: 2000 out of 5000 // loss:  tensor(29.2055, device='cuda:0')\n",
      "Iter: 2500 out of 5000 // loss:  tensor(28.2020, device='cuda:0')\n",
      "Iter: 3000 out of 5000 // loss:  tensor(28.2304, device='cuda:0')\n",
      "Iter: 3500 out of 5000 // loss:  tensor(27.6190, device='cuda:0')\n",
      "Iter: 4000 out of 5000 // loss:  tensor(26.8322, device='cuda:0')\n",
      "Iter: 4500 out of 5000 // loss:  tensor(26.5563, device='cuda:0')\n",
      "Iter: 5000 out of 5000 // loss:  tensor(26.3475, device='cuda:0')\n",
      "Training time:\n",
      " 0:47:01.527142\n"
     ]
    }
   ],
   "source": [
    "# gradient clipping, https://stackoverflow.com/questions/54716377/how-to-do-gradient-clipping-in-pytorch\n",
    "clip_value=1.\n",
    "for p in rnn.parameters():\n",
    "    p.register_hook(lambda grad: torch.clamp(grad, -clip_value, clip_value))\n",
    "\n",
    "optimizer = Adam(rnn.parameters(),lr=0.01) # 0.001 for RNN, 0.01 for LSTM and GRU work reasonably\n",
    "\n",
    "max_iter = 5000 # maximum number of training iterations\n",
    "current_iter = 0 # counter for training iterations\n",
    "\n",
    "# print the loss 11 times during the course of training\n",
    "print_idx_list = [1]+[int(np.floor(i*max_iter/10)) for i in range(1,11)] \n",
    "\n",
    "loop_bool = True\n",
    "\n",
    "if MODEL_STR == \"LSTM\":\n",
    "    h = ([torch.zeros(1, h_size, device=device) for i in range(depth)],[torch.zeros(1, h_size, device=device) for i in range(depth)])\n",
    "elif MODEL_STR == \"GRU\" or MODEL_STR == \"RNN\":\n",
    "    h = [torch.zeros(1, h_size, device=device) for i in range(depth)]\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "data = dset.textDataset(dset.text, train_size) # data: iterable with text given in chunks of size train_size (size counted by number of characters)\n",
    "train_dl = DataLoader(\n",
    "    data,\n",
    "    batch_size=batch_size,\n",
    "    pin_memory=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "rnn.train()\n",
    "t1 = datetime.datetime.now()\n",
    "while loop_bool:\n",
    "    for batch_txt in train_dl:\n",
    "        if current_iter<max_iter:\n",
    "            current_iter+=1\n",
    "            optimizer.zero_grad()\n",
    "            loss=utils.compute_loss(rnn,batch_txt,h,device)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if current_iter in print_idx_list:\n",
    "                print('Iter: {} out of {}'.format(current_iter,max_iter),'// loss: ',loss.detach())\n",
    "        else:\n",
    "            loop_bool = False\n",
    "            break\n",
    "t2 = datetime.datetime.now()\n",
    "print(\"Training time:\\n\",t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ingley had any consequence\n",
      "      away; but before he realons of the London; and you know he is appearance in the most ation as he\n",
      "      stood and make her three great\n",
      "      interest.”\n",
      "\n",
      "      “My dear Mr. Darcy, for she had not the messen, on better.”\n",
      "\n",
      "      “And but it was to deter him frequenting for\n",
      "      any comparison pourly a cross of his quiet readily anxiety to see you for it.”\n",
      "\n",
      "      “He is not a deep it is planess and a\n",
      "      dispostning, gentlemen provesed and\n",
      "      behaviour of nothing\n",
      "      encourageness than between the world I shall do you den sure in he was\n",
      "      contradied, and that they made her expected their\n",
      "      _tête-à-tête_ a be the\n",
      "      steady my dear beyond young our wishes, nor was Mr. Bingley and I will do the obliging what they achonsed.\n",
      "\n",
      "      Mr. Darcy believed, you know the whole windownce out as much and I have been proposals; for what I thought as before, seemed to listen a lively, for a lice him\n",
      "      only in Hertfordshire a gentlemen and the former concluded and Meryton Gard-nated\n",
      "      was\n",
      "      found herself he had one than she had been the\n",
      "      world. He is uncommon.\n",
      "\n",
      "      Colonel\n",
      "      “My dear, take not my dear. Well, Miss Benge!”\n",
      "\n",
      "      “She is raund\n",
      "      them. “There were, and She endeavouring\n",
      "      so,” said Elizabeth remembered not settled them\n",
      "      for\n",
      "      me to credit above say it was sest. My mother was an unyears and beauty, she was hore,\n",
      "      when I have understand.”\n",
      "\n",
      "      “That is\n",
      "      hels were in\n",
      "      Elizabeth, moreover,\n",
      "      Therefore stony to what had been together for her sister.\n",
      "\n",
      "      And that had perhaps have you as little way that had gained was exactly replied her\n",
      "      silence of her\n",
      "      was.\n",
      "\n",
      "      Mrs. Gardiner, and Mr. Bingley,” said she; “though, and how you mean and discourse us.”\n",
      "\n",
      "      “Why had been more\n",
      "      will them,\n",
      "      saying, had just be so arrangen be lady engaged.\n",
      "\n",
      "      Mrs. Hurst and Miss Bingley trind.——fown the overhour. He had\n",
      "      asked her, when Elizabeth, “but he i\n"
     ]
    }
   ],
   "source": [
    "utils.print_function(2000,rnn.to('cpu'),char_size,h_size,depth,MODEL_STR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
