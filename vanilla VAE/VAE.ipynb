{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE review\n",
    "\n",
    "## Problem setup\n",
    "Let $X=\\{x^{(i)}\\}^{N}_{i=1}$ be the dataset. \n",
    "We assume that the data is generated probabilistically from an unobserved continuous variable $z$ (called the latent variable) as follows: \n",
    "$z^{(i)}$ (value of the latent variable for the data $i$) is first sampled from the prior distribution $p_{\\theta^*}(z)$ ($\\theta^*$ is the true value of the parameter). Then, $x^{(i)}$ is sampled from the conditional distribution $p_{\\theta^*}(x|z)$.\n",
    "Here, we know neither the value of $\\theta^*$ nor the value of the latent variable $z^{(i)}$ corresponding to the observed value $x^{(i)}$.\n",
    "\n",
    "For the above sampling process, the distribution of the data is given by the marginal likelihood $p_\\theta(x) = \\int p_\\theta(z) p_\\theta(x|z)dz$.\n",
    "Unfortunately, the integral is intractable.\n",
    "The posterior probability is given by $p_\\theta(z|x)=p_\\theta(x|z)p_\\theta(z)/p_\\theta(x)$, and it is also intractable.\n",
    "Therefore, the EM algorithm cannot be used (Recall that in the EM algorithm, in the E step we compute the posterior probability (and put $Q_{i}(z^{(i)})=p(z^{(i)}|x^{(i)};\\theta)$) and in the M step we maximize $\\sum_{i,z^{(i)}}Q_{i}(z^{(i)}) \\log p(x^{(i)},z^{(i)};\\theta)/Q_i(z^{(i)})$)\n",
    "\n",
    "We also assume a large dataset, so that updating parameters is costly (Monte Carlo methods for computing $Q$ is also too slow)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational solution:\n",
    "\n",
    "We want to find $\\theta$ such that $p_\\theta(x^{(1)},...,p_\\theta(x^{(N)})$ is maximized (maximum likelihood).\n",
    "Instead of directly finding $\\theta$ that maximizes this, we maximize a lower bound by introducing $q_\\phi (z|x)$ which approximates the posterior probability $p_\\theta(z|x)$.\n",
    "\n",
    "1. Note that $\\log p_\\theta(x^{(1)})...p_\\theta(x^{(N)}) = \\sum_{i=1}^N \\log p_\\theta(x^{(i)}) = \\sum_{i=1}^N \\sum_{z} q_\\phi (z|x^{(i)}) \\log \\frac{p_\\theta (x^{(i)}|z) p_\\theta(z)}{p_\\theta(z|x^{(i)})} \\frac{q_\\phi (z|x^{(i)})}{q_\\phi (z|x^{(i)})} = \\sum_{i=1}^{N}[ E_{z\\sim q} [\\log p_\\theta(x^{(i)}|z)] - D_{KL}(q_\\phi(z|x^{(i)})|| p_\\theta(z)) + D_{KL}(q_\\phi (z|x^{(i)})|| p_\\theta(z|x^{(i)})) ]$\n",
    "\n",
    "2. Define $L(\\theta, \\phi; x^{(i)}) = E_{z\\sim q} [\\log p_\\theta (x^{(i)}|z)] - D_{KL}(q_\\phi(z|x^{(i)})|| p_\\theta(z))$.\n",
    "$L$ is tractable, but $D_{KL}(q_\\phi (z|x^{(i)}) || p_\\theta(z|x^{(i)})) $ is not (recall that posterior probability distribution is intractible).\n",
    "Since $D_{KL}(q_\\phi (z|x^{(i)}) || p_\\theta(z|x^{(i)})) \\ge 0$, we maximize $L$ (lower bound of $p_\\theta$) instead of $p_\\theta(x^{(1)},...,x^{(N})$.\n",
    "Note that this can be interpreted as maximizing $\\log p_\\theta(x^{(1)},...,p_\\theta(x^{(N)})-D_{KL}(q_\\phi (z|x^{(i)}) || p_\\theta(z|x^{(i)}))$.\n",
    "Thus, we are roughly maximizing $p_\\theta(x^{(1)},...,p_\\theta(x^{(N)})$ while minimizing $D_{KL}(q_\\phi (z|x^{(i)})||p_\\theta(z|x^{(i)}))$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to actually carry out the maximization process (gradient descent), we need to comment on the term $E_{z\\sim q} [\\log p_\\theta(x^{(i)}|z)]$.\n",
    "To minimize this value, we need to compute its gradient.\n",
    "However, the naive Monte Carlo approximation for the gradient $\\nabla_\\phi E_{q_\\phi(z)} [f(z)] = E_{q_\\phi(z)}[f(z) \\nabla_\\phi \\log q_\\phi (z)]$ exhibits a high variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. This problem of computing the gradient is circumvented by using the reparametrization trick. Instead of using random variable $z$, which is distributed according to $q_\\phi (z|x^{(i)})$, we assume that $z=g_\\phi (\\epsilon,x)$ ($g$ is some function), where $\\epsilon$ is a random variable which is distribued according to $p(\\epsilon)$. Thus, $z$ is determined once we know $\\epsilon$ and $x$.\n",
    "Then, the Monte Carlo estimate is $E_{q_\\phi(z|x^{(i)})}[f(z)]] = E_{p(\\epsilon)}[f(g_\\phi (\\epsilon,x^{(i)}))] \\approx \\frac{1}{L} \\sum_{l=1}^{L} f(g_\\phi (\\epsilon^{(l)},x^{(i)}))$.\n",
    "This yields the following estimate for L:\n",
    "$L(\\theta,\\phi;x^{(i)}) \\approx L^A (\\theta, \\phi; x^{(i)}) = \\frac{1}{L}\\sum_{i=1}^{L} \\log p_\\theta(x^{(i)},z^{(i,l)})-\\log q_\\phi (z^{(i,l)}|x^{(i)})$\n",
    "where $z^{(i,l)} = g_\\phi (\\epsilon^{(i,l)},x^{(i)})$ and $\\epsilon \\sim p(\\epsilon)$\n",
    "\n",
    "Here, we note that it is often possible to analtyically compute the KL divergence term in $L$. \n",
    "In such a case, the sampling is not needed to find the gradient for the KL divergence term. This yields a second estimate for L:\n",
    "$L^{B}(\\theta,\\phi;x^{(i)})= -D_{KL} (q_\\phi (z|x^{(i)}) || p_\\theta(z)) + \\frac{1}{L} \\sum_{l=1}^{L} \\log p_\\theta (x^{(i)}|z^{(i,l)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now,suppose that we have a dataset $X$ with $N$ datapoints. Then, we can estimate L over the whole dataset based on minibatches $X^{M}$, consisting of M datapoints randomly drawn from $X$:\n",
    "$L(\\theta,\\phi;X) \\approx L^{M} (\\theta,\\phi;X^{M} = \\frac{N}{M} \\sum_{i=1}^{M} L(\\theta,\\phi;x^{(i)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the experiments in the paper, it is safe to choose $L=1$ as long as $M$ is large ($\\sim 100$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Variationa Auto-Encoder:\n",
    "Use a neural network for $q_\\phi (z|x)$.\n",
    "Let $p_\\theta(z)=N(z;0,I)$ (multivariate Gaussian distribution with mean $0$ and variance given by the identity)\n",
    "In the original paper, the authors choose $p_\\theta (x|z)$ to be a multivariate Gaussian (for real-valued data) or a Bernouille distribution (for binary data) whose distribution parameters are computed from z using a multi-layered perceptron (fully connected network with single hidden layer.)\n",
    "The true posterior probability $p_\\theta (z|x)$ is intractable, but we assume that it takes the form of approximate Gaussian distribution with diagonal covariance:\n",
    "$\\log q_\\phi (z|x^{(i)}) = \\log N(z;\\mu^{(i)},\\sigma^{2i}I)$, where $\\mu^{(i)}$ and $\\sigma^{(i)}$ are outputs of MLP.\n",
    "When we assume this form for $q_\\phi (z|x^{(i)})$, $z^{(i,l)}=g_\\phi(x^{(i)},\\epsilon^{(l)})=\\mu^{(i)}+\\sigma^{(i)}\\cdot \\epsilon^{(l)}$ (here, $\\cdot$ is elementwise product)\n",
    "\n",
    "With this setup, the KL divergence in L can be computed analytically. Thus, we have\n",
    "$L(\\theta, \\phi, x^{(i)}) \\approx \\frac{1}{2} \\sum_{j=1}^{J} [1+\\log (\\sigma_{j}^{(i)})^2 - (\\mu_j^{(i)})^2 - (\\sigma_j^{(i)})^2] + \\frac{1}{L} \\sum_{l=1}^{L} \\log p_\\theta (x^{(i)}|z^{(i,l)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For MNIST, we use Bernouille distribution for the decoder.\n",
    "Then, ($j$ is the pixel index, and there is a sum over pixel for the loss function) $\\log p_\\theta (x^{(i)}_j|z)=\\log \\theta_j^{x^{(i)}_j} (1-\\theta_j)^{1-x^{(i)}_j}$.\n",
    "Although $x^{(i)}_j$ should be 0 or 1 for Bernouille distribution, we use the pixel value of MNIST here for $x^{(i)}_j$. \n",
    "Then, the loss is given by binary cross entropy between image in MNIST and the probability outputed by decoder network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda.\n"
     ]
    }
   ],
   "source": [
    "# Setup environment\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "device = torch.device('cuda' if torch.cuda.is_available()\n",
    "                     else torch.device('cpu'))\n",
    "print(f\"Training on {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "batch_size=128\n",
    "\n",
    "# Note that the MNIST dataset takes values between 0 and 1. \n",
    "# There is no need to further normalize them. \n",
    "# (In the training loss, we treat the pixel values as the probability)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../MNIST_data', train=True, download=False,\n",
    "                    transform=transforms.Compose([\n",
    "                        transforms.ToTensor(),\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../MNIST_data', train=False, download=False,\n",
    "                    transform=transforms.Compose([\n",
    "                        transforms.ToTensor(),\n",
    "                    ])),\n",
    "    batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create VAE model\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self,x_dim,h1_dim,z_dim):\n",
    "        super().__init__()\n",
    "        # Encoder with single hidden layer (Need to output mean and log variance)\n",
    "        self.fc1 = nn.Linear(x_dim,h1_dim) \n",
    "        self.fc2 = nn.Linear(h1_dim,z_dim) \n",
    "        self.fc3 = nn.Linear(h1_dim,z_dim)\n",
    "        # Decoder with single hidden layer \n",
    "        self.fc4 = nn.Linear(z_dim,h1_dim)\n",
    "        self.fc5 = nn.Linear(h1_dim,x_dim)\n",
    "        \n",
    "    def encoder(self,x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        mu = self.fc2(h)\n",
    "        log_var = self.fc3(h)\n",
    "        return mu, log_var\n",
    "    \n",
    "    def decoder(self,z):\n",
    "        h = F.relu(self.fc4(z))\n",
    "        x_prob = torch.sigmoid(self.fc5(h))\n",
    "        return x_prob\n",
    "    \n",
    "    def z_sample(self, mu, log_var):\n",
    "        std = torch.exp(log_var/2.)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps*std+mu\n",
    "    \n",
    "    def forward(self,x):\n",
    "        mu,log_var = self.encoder(x.view(-1,28*28))\n",
    "        z = self.z_sample(mu,log_var)\n",
    "        x_prob = self.decoder(z)\n",
    "        return x_prob, mu, log_var\n",
    "model = VAE(x_dim=28*28,h1_dim = 500,z_dim=2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function\n",
    "def loss_fn(x,x_prob,mu,log_var):\n",
    "    recon_error = F.binary_cross_entropy(x_prob,x.view(-1,28*28),reduction='sum')\n",
    "    KL_loss = -0.5*torch.sum(1.+log_var-mu**2-torch.exp(log_var))\n",
    "    return recon_error + KL_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train using Adam\n",
    "optimizer = optim.Adam(model.parameters(), lr = 1e-3)\n",
    "def train_VAE(n_epochs, optimizer, model, loss_fn, train_loader,test=False):\n",
    "    \"\"\"If test == True, also computes test loss\"\"\"\n",
    "    from tqdm import tqdm\n",
    "    trn_loss_list = []\n",
    "    test_loss_list = []\n",
    "    batch_size_trn = train_loader.batch_size\n",
    "    batch_num_trn = train_loader.__len__()\n",
    "    batch_size_test = test_loader.batch_size\n",
    "    batch_num_test = test_loader.__len__()\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        model.train()\n",
    "        loss_train = 0.\n",
    "        for imgs, _ in tqdm(train_loader,desc=f'training/epoch: {epoch} of {n_epochs}'):\n",
    "            imgs = imgs.to(device)\n",
    "            x_prob, mu, log_var = model(imgs)\n",
    "            loss = loss_fn(imgs,x_prob,mu,log_var)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_train+=loss.item()\n",
    "        trn_loss_list.append(loss_train/(batch_size_trn*batch_num_trn))\n",
    "\n",
    "        if test:\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                loss_test = 0.\n",
    "                for imgs, _ in tqdm(test_loader, desc=f'test/epoch: {epoch} of {n_epochs}'):\n",
    "                    imgs = imgs.to(device)\n",
    "                    x_prob, mu, log_var = model(imgs)\n",
    "                    loss = loss_fn(imgs,x_prob,mu,log_var)\n",
    "                    loss_test += loss.item()\n",
    "                test_loss_list.append(loss_test/(batch_size_test*batch_num_test))\n",
    "    return trn_loss_list, test_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training/epoch: 1 of 30: 100%|███████████████| 469/469 [00:15<00:00, 29.41it/s]\n",
      "test/epoch: 1 of 30: 100%|█████████████████████| 79/79 [00:01<00:00, 63.59it/s]\n",
      "training/epoch: 2 of 30: 100%|███████████████| 469/469 [00:15<00:00, 30.57it/s]\n",
      "test/epoch: 2 of 30: 100%|█████████████████████| 79/79 [00:01<00:00, 67.97it/s]\n",
      "training/epoch: 3 of 30: 100%|███████████████| 469/469 [00:15<00:00, 30.74it/s]\n",
      "test/epoch: 3 of 30: 100%|█████████████████████| 79/79 [00:01<00:00, 66.82it/s]\n",
      "training/epoch: 4 of 30: 100%|███████████████| 469/469 [00:15<00:00, 30.90it/s]\n",
      "test/epoch: 4 of 30: 100%|█████████████████████| 79/79 [00:01<00:00, 60.43it/s]\n",
      "training/epoch: 5 of 30: 100%|███████████████| 469/469 [00:15<00:00, 30.15it/s]\n",
      "test/epoch: 5 of 30: 100%|█████████████████████| 79/79 [00:01<00:00, 66.65it/s]\n",
      "training/epoch: 6 of 30: 100%|███████████████| 469/469 [00:15<00:00, 29.91it/s]\n",
      "test/epoch: 6 of 30: 100%|█████████████████████| 79/79 [00:01<00:00, 67.79it/s]\n",
      "training/epoch: 7 of 30: 100%|███████████████| 469/469 [00:15<00:00, 30.09it/s]\n",
      "test/epoch: 7 of 30: 100%|█████████████████████| 79/79 [00:01<00:00, 66.43it/s]\n",
      "training/epoch: 8 of 30: 100%|███████████████| 469/469 [00:15<00:00, 30.29it/s]\n",
      "test/epoch: 8 of 30: 100%|█████████████████████| 79/79 [00:01<00:00, 66.48it/s]\n",
      "training/epoch: 9 of 30: 100%|███████████████| 469/469 [00:15<00:00, 31.17it/s]\n",
      "test/epoch: 9 of 30: 100%|█████████████████████| 79/79 [00:01<00:00, 71.41it/s]\n",
      "training/epoch: 10 of 30: 100%|██████████████| 469/469 [00:15<00:00, 30.39it/s]\n",
      "test/epoch: 10 of 30: 100%|████████████████████| 79/79 [00:01<00:00, 65.82it/s]\n",
      "training/epoch: 11 of 30: 100%|██████████████| 469/469 [00:15<00:00, 30.79it/s]\n",
      "test/epoch: 11 of 30: 100%|████████████████████| 79/79 [00:01<00:00, 65.49it/s]\n",
      "training/epoch: 12 of 30: 100%|██████████████| 469/469 [00:15<00:00, 30.85it/s]\n",
      "test/epoch: 12 of 30: 100%|████████████████████| 79/79 [00:01<00:00, 70.90it/s]\n",
      "training/epoch: 13 of 30: 100%|██████████████| 469/469 [00:15<00:00, 30.65it/s]\n",
      "test/epoch: 13 of 30: 100%|████████████████████| 79/79 [00:01<00:00, 68.38it/s]\n",
      "training/epoch: 14 of 30: 100%|██████████████| 469/469 [00:14<00:00, 31.50it/s]\n",
      "test/epoch: 14 of 30: 100%|████████████████████| 79/79 [00:01<00:00, 71.28it/s]\n",
      "training/epoch: 15 of 30: 100%|██████████████| 469/469 [00:15<00:00, 30.64it/s]\n",
      "test/epoch: 15 of 30: 100%|████████████████████| 79/79 [00:01<00:00, 68.62it/s]\n",
      "training/epoch: 16 of 30: 100%|██████████████| 469/469 [00:15<00:00, 31.18it/s]\n",
      "test/epoch: 16 of 30: 100%|████████████████████| 79/79 [00:01<00:00, 70.90it/s]\n",
      "training/epoch: 17 of 30: 100%|██████████████| 469/469 [00:15<00:00, 30.87it/s]\n",
      "test/epoch: 17 of 30: 100%|████████████████████| 79/79 [00:01<00:00, 62.73it/s]\n",
      "training/epoch: 18 of 30: 100%|██████████████| 469/469 [00:15<00:00, 30.80it/s]\n",
      "test/epoch: 18 of 30: 100%|████████████████████| 79/79 [00:01<00:00, 70.90it/s]\n",
      "training/epoch: 19 of 30: 100%|██████████████| 469/469 [00:15<00:00, 30.58it/s]\n",
      "test/epoch: 19 of 30: 100%|████████████████████| 79/79 [00:01<00:00, 66.88it/s]\n",
      "training/epoch: 20 of 30: 100%|██████████████| 469/469 [00:15<00:00, 30.65it/s]\n",
      "test/epoch: 20 of 30: 100%|████████████████████| 79/79 [00:01<00:00, 63.74it/s]\n",
      "training/epoch: 21 of 30: 100%|██████████████| 469/469 [00:15<00:00, 30.03it/s]\n",
      "test/epoch: 21 of 30: 100%|████████████████████| 79/79 [00:01<00:00, 71.47it/s]\n",
      "training/epoch: 22 of 30: 100%|██████████████| 469/469 [00:15<00:00, 30.38it/s]\n",
      "test/epoch: 22 of 30: 100%|████████████████████| 79/79 [00:01<00:00, 66.37it/s]\n",
      "training/epoch: 23 of 30: 100%|██████████████| 469/469 [00:15<00:00, 29.62it/s]\n",
      "test/epoch: 23 of 30: 100%|████████████████████| 79/79 [00:01<00:00, 68.68it/s]\n",
      "training/epoch: 24 of 30: 100%|██████████████| 469/469 [00:15<00:00, 31.23it/s]\n",
      "test/epoch: 24 of 30: 100%|████████████████████| 79/79 [00:01<00:00, 67.45it/s]\n",
      "training/epoch: 25 of 30: 100%|██████████████| 469/469 [00:15<00:00, 30.22it/s]\n",
      "test/epoch: 25 of 30: 100%|████████████████████| 79/79 [00:01<00:00, 69.04it/s]\n",
      "training/epoch: 26 of 30: 100%|██████████████| 469/469 [00:15<00:00, 30.98it/s]\n",
      "test/epoch: 26 of 30: 100%|████████████████████| 79/79 [00:01<00:00, 64.95it/s]\n",
      "training/epoch: 27 of 30: 100%|██████████████| 469/469 [00:14<00:00, 31.57it/s]\n",
      "test/epoch: 27 of 30: 100%|████████████████████| 79/79 [00:01<00:00, 66.37it/s]\n",
      "training/epoch: 28 of 30: 100%|██████████████| 469/469 [00:15<00:00, 31.25it/s]\n",
      "test/epoch: 28 of 30: 100%|████████████████████| 79/79 [00:01<00:00, 65.22it/s]\n",
      "training/epoch: 29 of 30: 100%|██████████████| 469/469 [00:15<00:00, 31.22it/s]\n",
      "test/epoch: 29 of 30: 100%|████████████████████| 79/79 [00:01<00:00, 61.94it/s]\n",
      "training/epoch: 30 of 30: 100%|██████████████| 469/469 [00:15<00:00, 30.09it/s]\n",
      "test/epoch: 30 of 30: 100%|████████████████████| 79/79 [00:01<00:00, 64.79it/s]\n"
     ]
    }
   ],
   "source": [
    "train_loss_list, test_loss_list = train_VAE(n_epochs=30,optimizer=optimizer,model=model,loss_fn=loss_fn,train_loader=train_loader,test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxAAAAFgCAYAAAArRJ8VAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3dcbhcdX3v+/cnCYIgCoXYiyKGWAkawSCBQ6tYQltFPCl6qq1YWtprRbnqqfbKsdzntKBPe2up1tbaYvGaorWiVNGallZLHw1tL0oDBCSaXCDQuoVjAhQUEZTs7/1j1pZN2DtZe2evmT2z36/nmWfN/GbN7O9izPr6Xb/1+/1SVUiSJElSG4sGHYAkSZKk4WEBIUmSJKk1CwhJkiRJrVlASJIkSWrNAkKSJElSa0sGHcDeOPTQQ2vZsmWDDkOSht511113d1UtHXQcc8HcIElzY7rcMNQFxLJly9i4ceOgw5CkoZfk3wcdw1wxN0jS3JguN3gLkyRJkqTWLCAkSZIktWYBIUmSJKm1oR4DIUlz5Qc/+AFjY2M89NBDgw6lU/vttx+HH344++yzz6BDkaShMsp5Yqa5wQJCkoCxsTEOPPBAli1bRpJBh9OJquKee+5hbGyMI488ctDhSNJQGdU8MZvc4C1MkgQ89NBDHHLIISOVFHaVhEMOOWQkr55JUtdGNU/MJjdYQEhSY9SSwlQWwjFKUldG9Rw60+OygJAkSZLUmgWEJM0D9913H3/2Z38248+dfvrp3HfffR1EJEmab2abKwD+6I/+iAcffHBO4rCAkKR5YLqksHPnzt1+7sorr+Sggw7qKixJ0jxiATFI27bBypWwZElvu23boCOSNGzm+Dzym7/5m9x2222sWrWKE044gTVr1vDa176WY445BoBXvOIVHH/88axcuZJLLrnkh59btmwZd999N3fccQfPec5zeP3rX8/KlSt5yUtewve+9729iqkLSdYl2Z7k5l3a35Jka5LNSS6a1H5skmua9q8m2a+LuEwLkuZaF+eVybnivPPO4w/+4A844YQTOPbYY7ngggsA+O53v8vLX/5ynv/85/O85z2PT37yk7z//e/nzjvvZM2aNaxZs2bvA6mqoX0cf/zxNSvPfW7VokVV0Ns+97mz+x5JI+NrX/vazD4wx+eR22+/vVauXFlVVV/84hdr//33r23btv3w/Xvuuaeqqh588MFauXJl3X333VVV9cxnPrN27NhRt99+ey1evLhuuOGGqqp69atfXX/5l3855d+a6liBjdWH8zbwYuAFwM2T2tYAVwH7Nq+f2myXADcBz29eHwIs3tPfmE1uMC1I2pOZ5okuziuTc8XnP//5ev3rX1/j4+O1c+fOevnLX14bNmyoT33qU/Vrv/ZrP/zMfffdV1WP5ovpzCQ3LMweiK1bYXy893x8vPdakmai4/PIiSee+Jj5uN///vfz/Oc/n5NOOolvfOMb3HLLLY/7zJFHHsmqVasAOP7447njjjvmNKa5UFVXA/fu0nwu8O6qerjZZ3vT/hLgpqq6sWm/p6p2f0/XLJkWJM21rs8rX/jCF/jCF77Acccdxwte8AK2bNnCLbfcwjHHHMNVV13FO97xDv75n/+ZpzzlKXP7h1motzCtWAGLmkNftKj3WpJmouPzyAEHHPDD51/60pe46qqruOaaa7jxxhs57rjjppyve9999/3h88WLF/PII4/MaUwdOgo4OclXkmxIcsKk9kry+STXJ/kf031BknOSbEyycceOHTMOwLQgaa51fV6pKs4//3w2bdrEpk2buPXWW3nd617HUUcdxXXXXccxxxzD+eefz7ve9a65/cMs1AJi/Xo4+mhYvLi3Xb9+0BFJGjZzfB458MAD+c53vjPle/fffz8HH3ww+++/P1u2bOHLX/7yXv2teWgJcDBwEnAecHl6k5IvAV4E/GKzfWWSn5rqC6rqkqpaXVWrly5dOuMATAuS5loX55XJueKlL30p69at44EHHgDgm9/8Jtu3b+fOO+9k//3356yzzuLtb387119//eM+u7eWzMm3DJvly2Hz5kFHIWmYzfF55JBDDuGFL3whz3ve83jiE5/Ij/7oj/7wvdNOO40PfvCDHHvssaxYsYKTTjppzv7uPDEGXNHcb3ttknHg0KZ9Q1XdDZDkSnrjJ/5prgMwLUiaa12cVybnipe97GW89rWv5cd//McBeNKTnsTHPvYxbr31Vs477zwWLVrEPvvsw8UXXwzAOeecw8te9jIOO+wwvvjFL+5VHOmdr4fT6tWra+PGjYMOQ9II+PrXv85znvOcQYfRF1Mda5Lrqmp1P/5+kmXA31bV85rXbwSeVlW/neQoegXCEcBBzfMXAd8H/gF4X1X93e6+39wgqQujnidmkhsWZg+EJGkgklwGnAIcmmQMuABYB6xrpnb9PnB20xvxn0n+EPg3oIAr91Q8SJK6ZwEhSeqbqjpzmrfOmmb/jwEf6y4iSdJMLcxB1JI0hWG+pbOthXCMktSVUT2HzvS4LCAkCdhvv/245557RjY5QC9B3HPPPey3XyeLOUvSSBvVPDGb3OAtTJIEHH744YyNjTGbNQSGyX777cfhhx8+6DAkaeiMcp6YaW6wgJAkYJ999nnMys+SJE1mnnhUZ7cwJVmXZHszq8ZE26okX06yqVkx9MSm/ZQk9zftm5L8dldxSZIkSZq9LsdAXAqctkvbRcA7q2oV8NvN6wn/XFWrmsfcr7ktSZIkaa91VkBU1dXAvbs2A09unj8FuLOrvy9JkiRp7vV7DMRbgc8neQ+94uUnJr3340lupFdUvL2q5njxb0mSJEl7q9/TuJ4LvK2qngG8Dfhw03498Myqej7wJ8Bnp/uCJOc04yc2juIoeEmSJGk+63cBcTZwRfP8r4ETAarq21X1QPP8SmCfJIdO9QVVdUlVra6q1UuXLu1HzJIkSZIa/S4g7gR+snl+KnALQJL/LUma5yc2cd3T59gkSZIk7UFnYyCSXAacAhyaZAy4AHg98MdJlgAPAec0u78KODfJI8D3gNfUqC3zJ0mSJI2AzgqIqjpzmreOn2LfDwAf6CoWSZIkSXOj37cwSZIkSRpiFhCSJEmSWrOAkCRJktSaBYQkSZKk1iwgJEmSJLVmASFJkiSpNQsISZIkSa1ZQEiSJElqzQJCkiRJUmsWEJIkSZJas4CQJEmS1JoFhCRJkqTWLCAkSZIktWYBIUmSJKk1CwhJkiRJrVlASJIkSWrNAkKS1DdJ1iXZnuTmXdrfkmRrks1JLtrlvSOSPJDk7f2NVpI0FQsISVI/XQqcNrkhyRrgDODYqloJvGeXz7wP+Pu+RCdJ2qMlgw5AkrRwVNXVSZbt0nwu8O6qerjZZ/vEG0leAWwDvtuvGCVJu2cPhCRp0I4CTk7ylSQbkpwAkOQA4B3AOwcanSTpMeyBkCQN2hLgYOAk4ATg8iTL6RUO76uqB5Ls9guSnAOcA3DEEUd0G60kLXAWEJKkQRsDrqiqAq5NMg4cCvwX4FXNoOqDgPEkD1XVB3b9gqq6BLgEYPXq1dW/0CVp4bGAkCQN2meBU4EvJTkKeAJwd1WdPLFDkguBB6YqHiRJ/WUBIUnqmySXAacAhyYZAy4A1gHrmqldvw+c3fRGSJLmIQsISVLfVNWZ07x11h4+d+HcRyNJmg1nYZIkSZLUmgWEJEmSpNYsICRJkiS1ZgEhSZIkqbVOC4gk65Jsb2bWmGhbleTLSTYl2ZjkxKY9Sd6f5NYkNyV5QZexSZIkSZq5rnsgLgVO26XtIuCdVbUK+O3mNcDLgGc3j3OAizuOTZIkSdIMdVpAVNXVwL27NgNPbp4/BbizeX4G8NHq+TJwUJLDuoxPkiRJ0swMYh2ItwKfT/IeegXMTzTtTwe+MWm/sabtrskfTnIOvR4KjjjiiM6DlSRJkvSoQQyiPhd4W1U9A3gb8OGmPVPs+7iVSKvqkqpaXVWrly5d2mGYkiRJknY1iALibOCK5vlfAyc2z8eAZ0za73Aevb1JkiRJ0jwwiALiTuAnm+enArc0zz8H/HIzG9NJwP1VdddUXyBJkiRpMDodA5HkMuAU4NAkY8AFwOuBP06yBHiIZjwDcCVwOnAr8CDwq13GJkmSJGnmOi0gqurMad46fop9C3hTl/FIkiRJ2juuRC1JkiSpNQsISZIkSa1ZQEiSJElqzQJCkiRJUmsWEJIkSZJas4CQJEmS1JoFhCRJkqTWLCAkSZIktWYBIUmSJKk1CwhJkiRJrVlASJIkSWrNAkKSJElSaxYQkiRJklqzgJAkSZLUmgWEJEkzsG0brFwJS5b0ttu2DToiSeovCwhJkmZg7VrYsgV27uxt164ddESS1F8WEJIkzcDWrTA+3ns+Pt57LUkLiQWEJKlvkqxLsj3Jzbu0vyXJ1iSbk1zUtP1MkuuSfLXZnjqYqB9rxQpY1GTPRYt6ryVpIbGAkCT106XAaZMbkqwBzgCOraqVwHuat+4G1lbVMcDZwF/2Mc5prV8PRx8Nixf3tuvXDzoiSeqvJYMOQJK0cFTV1UmW7dJ8LvDuqnq42Wd7s71h0j6bgf2S7Dux36AsXw6bNw8yAkkaLHsgJEmDdhRwcpKvJNmQ5IQp9vk54Ibpiock5yTZmGTjjh07Og1WkhY6CwhJ0qAtAQ4GTgLOAy5Pkok3k6wEfh94w3RfUFWXVNXqqlq9dOnSruOVpAXNAkKSNGhjwBXVcy0wDhwKkORw4DPAL1fVbQOMUZLUsICQJA3aZ4FTAZIcBTwBuDvJQcDfAedX1b8OMD5J0iQWEJKkvklyGXANsCLJWJLXAeuA5c3Urp8Azq6qAt4M/BjwW0k2NY+nDix4SRLgLEySpD6qqjOneeusKfb9HeB3uo1IkjRT9kBIkiRJas0CQpIkSVJrnRUQSdYl2d7c0zrR9slJ97HekWRT074syfcmvffBruKSJEmSNHtdjoG4FPgA8NGJhqr6hYnnSd4L3D9p/9uqalWH8UiSJEnaS50VEFV1dZJlU73XLBD08zTT9kmSJEkaDoMaA3Ey8K2qumVS25FJbkiyIcnJ030wyTlJNibZuGPHju4jlSRJkvRDgyogzgQum/T6LuCIqjoO+A3g40mePNUHq+qSqlpdVauXLl3ah1AlSZIkTeh7AZFkCfDfgE9OtFXVw1V1T/P8OuA24Kh+xyZJkiRp9wbRA/HTwJaqGptoSLI0yeLm+XLg2cC2AcQmSZIkaTe6nMb1MuAaYEWSsSSva956DY+9fQngxcBNSW4EPgW8saru7So2SZIkSbPT5SxMZ07T/itTtH0a+HRXsUiSJEmaG65ELUmSJKk1CwhJkiRJrVlASJIkSWrNAkKSJElSaxYQkiRJklqzgJAkSZLUmgWEJEmSpNYsICRJkiS1ZgEhSZIkqTULCEmSJEmtWUBIkiRJas0CQpIkSVJrFhCSJEmSWrOAkCRJktSaBYQkSR3Ztg1WroQlS3rbbdsGHZEk7T0LCEmSOrJ2LWzZAjt39rZr1w46IknaexYQkiR1ZOtWGB/vPR8f772WpGFnASFJUkdWrIBFTaZdtKj3WpKGnQWEJKlvkqxLsj3Jzbu0vyXJ1iSbk1w0qf38JLc27720/xHvnfXr4eijYfHi3nb9+kFHJEl7b8mgA5AkLSiXAh8APjrRkGQNcAZwbFU9nOSpTftzgdcAK4GnAVclOaqqdvY96llavhw2bx50FJI0t+yBkCT1TVVdDdy7S/O5wLur6uFmn+1N+xnAJ6rq4aq6HbgVOLFvwUqSpmQBIUkatKOAk5N8JcmGJCc07U8HvjFpv7Gm7XGSnJNkY5KNO3bs6DhcSVrYLCAkSYO2BDgYOAk4D7g8SYBMsW9N9QVVdUlVra6q1UuXLu0uUkmSBYQkaeDGgCuq51pgHDi0aX/GpP0OB+4cQHySpEn2WEAkOSDJoub5UUl+Nsk+3YcmSVogPgucCr08AzwBuBv4HPCaJPsmORJ4NnDtwKKUJAHteiCuBvZL8nTgn4BfpTeLhiRJM5LkMuAaYEWSsSSvA9YBy5upXT8BnN30RmwGLge+BvwD8KZhmoFJkkZVm2lcU1UPNif5P6mqi5Lc0HVgkqTh0PRSP6mqvr2nfavqzGneOmua/X8X+N29CE+SNMfa9EAkyY8Dvwj8XdPm+hGStIAl+XiSJyc5gF4PwdYk5w06LklS99oUEG8Fzgc+U1WbkywHvrinD0212miSTybZ1DzuSLJp0ntDvdqoJC0wz216HF4BXAkcAfzSYEOSJPXDHnsSqmoDsAF+2E19d1X99xbffSm7rDZaVb8w8TzJe4H7m+dDv9qoJC0w+zQTarwC+EBV/SDJlFOsSpJGS5tZmGbVTT3NaqMT3xng54HLmiZXG5Wk4fLnwB3AAcDVSZ4J7HEMhCRp+LW5hamLbuqTgW9V1S3Na1cblaQhUlXvr6qnV9XpzYxJ/w6sGXRckqTutSkgJndT/01V/YBpVgKdgTN5tPcBXG1UkoZKkl9veqeT5MNJrqdZy0GSNNraFBBz2k2dZAnw34BPTmp2tVFJGi7/e9M7/RJgKb01gt492JAkSf2wxwKig27qnwa2VNXYpDZXG5Wk4TLRc3w68BdVdSNT9yZLkkZMm0HUT0nyhxPjDprZkw5o8bmpVhuF3mxLk29fwtVGJWnoXJfkC/QKiM8nORAYH3BMkqQ+aLMg3DrgZnqzJkFvAPVf0LsNaVrTrTZaVb8yTburjUrS8HgdsArYVlUPJjmE3m1MkqQR16aAeFZV/dyk1++cvACcJGnhqarxJIcDr+3NzM2Gqlo/4LAkSX3QZhD195K8aOJFkhcC3+suJEnSfJfk3cCv07v19GvAf0/ye4ONanht2wYrV8KSJb3ttm2DjkiSptemB+Jc4CNJnkJvgNy9wK90GZQkad47HVhVVeMAST4C3ACcP9CohtTatbBlC4yP97Zr18LmzYOOSpKmtscCoqo2Ac9P8uTmtSuNSpIADqJ3UQngKYMMZNht3dorHqC33bp1sPFI0u5MW0Ak+Y1p2gGoqj/sKCZJ0vz3e8ANSb5Ir3f6xdj7MGsrVjzaA7FoUe+1JM1Xu+uBOLBvUUiShkpVXZbkS8AJ9AqId1TV/xpsVMNr/frebUtbt/aKh/UOR5c0j01bQFTVO/sZiCRp/kvygl2aJhYFfVqSp1XV9f2OaRQsX+6YB0nDo80gakmSJrx3N+8VcGq/ApEkDYYFhCSptapaM+gYJEmD1WYdCEmSJEkCWvRAJNkX+Dlg2eT9q+pd3YUlSZIkaT5qcwvT3wD3A9cBD3cbjiRJkqT5rE0BcXhVndZ5JJKkoZHkn6rqp/bUJkkaPW0KiP83yTFV9dXOo5EkzWtJ9gP2Bw5NcjC9NSAAngw8bWCBSZL6pk0B8SLgV5LcTu8WpgBVVcd2GpkkaT56A/BWesXCdTxaQHwb+NNBBSVJ6p82BcTLOo9CkjQUquqPgT9O8paq+pNBxyNJ6r9pp3FN8uTm6XemeUiSFq7/leRAgCT/M8kVU6xSLUkaQbtbB+LjzfY6YGOzvW7Sa0nSwvVbVfWdJC8CXgp8BLh4wDFJkvpg2luYquq/Ntsj+xeOJGlI7Gy2Lwcurqq/SXLhAOORJPVJq5Wokxyc5MQkL554dB2YJGle+2aSPwd+HriyWXR0jzklybok25PcPKntwiTfTLKpeZzetO+T5CNJvprk60nO7+xoJEmttTnZ/xpwNfB54J3N9sJuw5IkzXM/Ty8fnFZV9wE/ApzX4nOXAlOtLfS+qlrVPK5s2l4N7FtVxwDHA29IsmxvA5ck7Z02PRC/DpwA/HtVrQGOA3Z0GpUkaV6rqgeB7fSm+gZ4BLilxeeuBu5t+2eAA5IsAZ4IfJ/edLGSpAFqU0A8VFUPASTZt6q2ACu6DUuSNJ8luQB4BzBxW9E+wMf24ivfnOSm5hang5u2TwHfBe4C/gN4T1VNWXwkOSfJxiQbd+zwGpckdalNATGW5CDgs8A/Jvkb4M5uw5IkzXOvBH6W3v/Bp6ruBA6c5XddDDwLWEWvWHhv034ivcHaTwOOBP7PJMun+oKquqSqVlfV6qVLl84yjOGwbRusXAlLlvS227YNOiJJC80eC4iqemVV3VdVFwK/BXwYeEXXgc0LnqUlaTrfr6qid5sRSQ6Y7RdV1beqamdVjQMfolc4ALwW+Ieq+kFVbQf+FVi9l3EPvbVrYcsW2Lmzt127dvp9TWOSurDbAiLJoskzZVTVhqr6XFV9v/vQ5oGZnKUlaWG5vJmF6aAkrweuAv6f2XxRksMmvXwlMJF3/gM4NT0HACcBW/Yi5pGwdSuMj/eej4/3Xk/HNCapC9OuAwFQVeNJbkxyRFX9R7+CmjdmcpaWpAWkqt6T5GfoDWpeAfx2Vf3jnj6X5DLgFODQJGPABcApSVbR6824A3hDs/ufAn9Br6AI8BdVddMcH8rQWbGiVwyMj8OiRb3X0zGNSerCbguIxmHA5iTX0tzrClBVP9tZVPPFTM7SkrSAJPn9qnoH8I9TtE2rqs6covnD0+z7AL2pXDXJ+vW9noStW3tpaf366fc1jUnqQptB1O8E/ivwLnoD2yYeuzXVYkFN+1uSbE2yOclFTduyJN+btIjQB2d+KB1Yvx6OPhoWL+5td3eWlqSF5WemaHtZ36NYgJYvh82b4ZFHetvlUw4r7zGNSepCmx6I03e9opTk94ENe/jcpcAHgI9O+twa4Azg2Kp6OMlTJ+1/W1WtahV1v0ycpSVJACQ5F/g/gOVJJt9OdCC9Qc6aR0xjkrrQpgdiVleZplks6Fzg3VX1cLPP9hZ/X5I0f3wcWAt8rtlOPI6vqrMGGZgkqT+mLSCSnJvkq8CKZnGficftwGwHsR0FnJzkK0k2JDlh0ntHJrmhaT95N3G5WJAkDUhV3V9Vd1TVmVX175MebVeXliQNud3dwvRx4O+B3wN+c1L7d/YiUSwBDqY3Fd8J9KYBXE5v4aAjquqeJMcDn02ysqq+vesXVNUlwCUAq1evrlnGIUmSJGkWpi0gqup+4H5gqhkzZmsMuKJZfOjaJOPAoVW1A5i4rem6JLfR663YOId/W5IkSdJeajMGYi59FjgVIMlRwBOAu5MsTbK4aV8OPBtwvUxJkiRpnmkzC9OsTLNY0DpgXTO16/eBs6uqkrwYeFeSR4CdwBu9n1aSJEmafzorIKZZLAjgcbN0VNWngU93FYskSZKkudHvW5gkSZIkDTELCEmSJEmtWUBIkiRJas0CQpIkSVJrFhCSJEmSWrOAkCRJktSaBYQkSZKk1iwgJEmSJLVmATFXtm2DlSthyZLedtu2QUckSZIkzTkLiLmydi1s2QI7d/a2a9cOOiJJkiRpzllAzJWtW2F8vPd8fLz3WpIkSRoxFhBzZcUKWNT851y0qPdakiRJGjEWEHNl/Xo4+mhYvLi3Xb9+0BFJkiRJc84CYq4sXw6bN8Mjj/S2y5cPOiJJklpxHhBJM2EBIUnSAuc8IJJmwgJCkqQFznlAJM2EBYQkSQuc84BImgkLCEmSFjjnAZE0E0sGHYAkSRqsiXlAJKkNeyAkSVJrztgkyQJiEDz7SpKGlDM2SbKAGATPvpIWqCTrkmxPcvOktguTfDPJpuZx+qT3jk1yTZLNSb6aZL/BRK4JztgkyQJiEDz7Slq4LgVOm6L9fVW1qnlcCZBkCfAx4I1VtRI4BfhBvwLV1JyxSZIFxCB49pW0QFXV1cC9LXd/CXBTVd3YfPaeqtrZWXBqxRmbJFlADIJnX0na1ZuT3NTc4nRw03YUUEk+n+T6JP9jug8nOSfJxiQbd+zY0Z+IF6iJGZseeaS3Xb580BFJ6jcLiEHw7CtJk10MPAtYBdwFvLdpXwK8CPjFZvvKJD811RdU1SVVtbqqVi9durQPIUvSwmUBIUkaqKr6VlXtrKpx4EPAic1bY8CGqrq7qh4ErgReMKg4JUk9FhCSpIFKctikl68EJmZo+jxwbJL9mwHVPwl8rd/xSZIeq7MCYqqp+pr2tyTZ2kzJd9Gk9vOT3Nq899Ku4pIkDU6Sy4BrgBVJxpK8DriomaL1JmAN8DaAqvpP4A+BfwM2AddX1d8NKHTNgsseSaNpSYfffSnwAeCjEw1J1gBnAMdW1cNJntq0Pxd4DbASeBpwVZKjnG1DkkZLVZ05RfOHd7P/x+hN5aohNLHs0fj4o8sebd486Kgk7a3OeiCmmarvXODdVfVws8/2pv0M4BNV9XBV3Q7cyqP3wC5cXrqRJA0xlz2SRlO/x0AcBZyc5CtJNiQ5oWl/OvCNSfuNNW2Ps6Cm6nPFaknSEHPZI2k09buAWAIcDJwEnAdcniRApti3pvqCBTVVn5duJElDzGWPpNHU5RiIqYwBV1RVAdcmGQcObdqfMWm/w4E7+xzb/LNixaM3j3rpRpI0ZCaWPZI0WvrdA/FZ4FSAJEcBTwDuBj4HvCbJvkmOBJ4NXNvn2OYfL91IkiRpnumsB6KZqu8U4NAkY8AFwDpgXTO16/eBs5veiM1JLqc3v/cjwJucgQkv3UiSJGne6ayAmGaqPoCzptn/d4Hf7SoeSZIkSXvPlaglSZIktWYBMSpcM0KSJEl9YAExKlwzQpI0pLwGJg0XC4hR4ZoRkqQh5TUwabhYQIwKl/uUJA0pr4FJw8UCYlS4ZoQkaUh5DUwaLv1eiVpdcc0ISdKQWr++d9vS1q294sFrYNL8ZgEhSZIGymtg0nDxFqaFyOkuJEmSNEsWEAuR011IkiRpliwgFiKnu5AkSdIsWUAsRE53IUmSpFmygFiInPJVkiRJs+QsTAuR011IkiRpluyB0PScrUmSJEm7sIDQ9JytSZIkSbuwgND0nK1JkiRJu7CA0PScrUmSJEm7sIDQ9JytSZI0zzg8Txo8Z2HS9JytSZI0z0wMzxsff3R4nqlK6i97IDQ3vCQkSeqDtsPzTEtSdywgNDecsUmS1Adth+eZlqTuWEBobjhjk6QWkqxLsj3JzZPaLkzyzSSbmsfpu3zmiCQPJHl7/yPWfNN2eJ5pSeqOBYTmhjM2SWrnUuC0KdrfV1WrmseVu74H/H3nkWkoTAzPe+SR3nb58qn3M8VFfNMAAA6KSURBVC1J3bGA0NxwxiZJLVTV1cC9bfdP8gpgG+AwWc2IaUnqjgWE5kbbS0LgyDZJU3lzkpuaW5wOBkhyAPAO4J17+nCSc5JsTLJxx44dXceqITCTtCRpZiwg1H+ObJP0WBcDzwJWAXcB723a30nv1qYH9vQFVXVJVa2uqtVLly7tLlJJkutAaAAc2SZpkqr61sTzJB8C/rZ5+V+AVyW5CDgIGE/yUFV9YABhSpIanfVAzGSmjSTLknxvUvsHu4pL84Aj2yRNkuSwSS9fCdwMUFUnV9WyqloG/BHwf1s8SNLgdXkL06XMbKaN2ya1v7HDuDRobUe2OVZCGjlJLgOuAVYkGUvyOuCiJF9NchOwBnjbQIPUgmO6kWams1uYqurqJMu6+n4NsYmRbXsyMVZifPzRsRJtPidp3qqqM6do/nCLz10499FIPaYbaWYGMYj6cTNtNI5MckOSDUlOnu7DzrSxgDhWQpLUB6YbaWb6XUBMN9PGXcARVXUc8BvAx5M8eaovcKaNBcSxEpKkPjDdSDPT1wKiqr5VVTurahz4EHBi0/5wVd3TPL8OuA04qp+xaR6aySpA3sAqSZolF52TZqav07gmOayq7mpe/nCmjSRLgXurameS5cCz6a08qoWs7VgJ8AZWSdKszSTdSOp2GteZzLTxYuCmJDcCnwLeWFX3dhWbRpA3sEqS5hE7xjXKupyFqfVMG1X1aeDTXcWiBWDFikd7ILyBVZLUkW3bep3cW7f2Us369b0ejF3ZMa5RNohZmKS553gJSVIfTBQGO3c+WhhMxY5xjTILCI2GiRtYH3mkt53qctCEtmd/SZJ20bYwcGYnjTILCC08XhaSJM1S28LAmZ00yiwgtPC0Pft7q5MkaRdtC4OZdIybbjRsLCC08LQ9+3urkyRpFzMpDNoy3WjY9HUdCGleaDvht7c6SZL6wHSjYWMPhDSdmYyAs/9ZkjRLXaQb05K6ZAEhTWcmI+Dsf5YkzVIX6ca0pC55C5M0nba3OoH9z5KkWesi3ZiW1CV7IKS54MxOkqQ+aJtuXIdCXbKAkOaCMztJkvqgbbpxHQp1yVuYpLnQxcxO27b1CoytW3uXjtavn5v5AiVJQ6ttupnJbVHSTNkDIfXTTPqU7a2QJEnzkAWE1E8z6VOeaW+FYyskSVIfWEBI/TSTJUztrZAk9YHXoDRTFhDSfNVVb4UkSZPM5BqUxYbAAkKav7rorfDML0naxUyuQdnhLbCAkEaD08hKkmZpJnfM2uEtsICQRkPb3goHZkuSdjGTO2bt8BZYQEgLiwOzJUm7mMkds3Z4CywgpIXFaWQlSXuhiw5vDR8LCGkhcRpZSVIfzCSFaPhYQEiaWhe9FfZUSNKCMJMUouFjASFpal30VthTseAlWZdke5KbJ7VdmOSbSTY1j9Ob9p9Jcl2SrzbbUwcXuaSZmEkK8drS8LGAkLT32l5qclyF4FLgtCna31dVq5rHlU3b3cDaqjoGOBv4yz7FKKmPvLY0fCwgJO29tpeaHFex4FXV1cC9Lfe9oarubF5uBvZLsm9nwUkaCK8tDR8LCEn94yxQmt6bk9zU3OJ08BTv/xxwQ1U9PNWHk5yTZGOSjTt27Og2UklzymtLw8cCQlL/DHoWKAuN+epi4FnAKuAu4L2T30yyEvh94A3TfUFVXVJVq6tq9dKlS7uMVdIc6+rakrrTWQExk4FyzXvnJ7k1ydYkL+0qLklDoouM4qWreamqvlVVO6tqHPgQcOLEe0kOBz4D/HJV3TaoGCV1p6trS6NovlwH67IH4lJaDpRL8lzgNcDK5jN/lmRxh7FJmu+6yCjeFjUvJTls0stXAjc37QcBfwecX1X/OojYJM0vXUwPO0yn+/lyHayzAmImA+WAM4BPVNXDVXU7cCuTrkBJ0m61zSjeaDtwSS4DrgFWJBlL8jrgomaq1puANcDbmt3fDPwY8FuTeq6fOpjIJc0Hba8tzaQoGKbT/Xy5hWsQYyCmGij3dOAbk/YZa9oex4Fykh6nbUZxEPfAVdWZVXVYVe1TVYdX1Yer6peq6piqOraqfraq7mr2/Z2qOmBSr/Wqqto+6GOQNP/NpCiYL/+nvI35cgtXvwuI6QbKZYp9a6ovcKCcpFlzELckLQgzKQpmcrof9Gl8vqzw3dcCYjcD5caAZ0za9XDgzl0/L0l94yBuSRpaMykKZnK67+J60Uz2ncl1sC71tYCYbqAc8DngNUn2TXIk8Gzg2n7GJkmP4SBuSRpaMykKZnK67+J60TBeW+pyGtfWA+WqajNwOfA14B+AN1XVzq5ik6Q5NehB3BYbkvQYXV2p7+J6URdjMLpOC6macqjBUFi9enVt3Lhx0GFIUjvbtvUKga1be1ln/frps9qSJb3iYcLixb1MOJWVK3tFxvh4L6MdfXQvY85AkuuqavWMPjRPmRskdaXtaXwmp+U5OIV39p3T5QZXopakfulqEPcwTSEiSUOsi0n/uhgY3XVaWDK3XydJmhPr1z/+Mtd0Vqx47KWmhbY0qyTNMxOFxlzv21bXacEeCEmaj2bSWzFf5vWTJM0LXacFeyAkadh1cflKkjS0uk4L9kBIkiRJas0CQpIkSVJrFhCSJEmSWrOAkCRJktSaBYQkSZKk1iwgJEmSJLVmASFJkiSpNQsISZIkSa1ZQEiSJElqzQJCkiRJUmupqkHHMGtJdgD/vkvzocDdAwinS6N4TDCax+UxDYdRPCbYu+N6ZlUtnctgBsXcMNQ8puExisflMT3elLlhqAuIqSTZWFWrBx3HXBrFY4LRPC6PaTiM4jHB6B7XXBjF/zYe03AYxWOC0Twuj6k9b2GSJEmS1JoFhCRJkqTWRrGAuGTQAXRgFI8JRvO4PKbhMIrHBKN7XHNhFP/beEzDYRSPCUbzuDymlkZuDIQkSZKk7oxiD4QkSZKkjlhASJIkSWptpAqIJKcl2Zrk1iS/Oeh45kKSO5J8NcmmJBsHHc9sJFmXZHuSmye1/UiSf0xyS7M9eJAxzsY0x3Vhkm82v9emJKcPMsaZSvKMJF9M8vUkm5P8etM+tL/Xbo5paH+rJPsluTbJjc0xvbNpPzLJV5rf6ZNJnjDoWAdtFPMCmBvmK/PCcBjFvAD9zQ0jMwYiyWLg/wN+BhgD/g04s6q+NtDA9lKSO4DVVTW0C5skeTHwAPDRqnpe03YRcG9VvbtJ6gdX1TsGGedMTXNcFwIPVNV7BhnbbCU5DDisqq5PciBwHfAK4FcY0t9rN8f08wzpb5UkwAFV9UCSfYB/AX4d+A3giqr6RJIPAjdW1cWDjHWQRjUvgLlhvjIvDMdvNYp5AfqbG0apB+JE4Naq2lZV3wc+AZwx4JgEVNXVwL27NJ8BfKR5/hF6/3CHyjTHNdSq6q6qur55/h3g68DTGeLfazfHNLSq54Hm5T7No4BTgU817UP1O3XEvDCPjWJuMC8Mh1HMC9Df3DBKBcTTgW9Mej3GCPyPgd4P/4Uk1yU5Z9DBzKEfraq7oPcPGXjqgOOZS29OclPTlT00Xbq7SrIMOA74CiPye+1yTDDEv1WSxUk2AduBfwRuA+6rqkeaXUblHLg3RjUvgLlh2AztuWYy88L816/cMEoFRKZoG4X7s15YVS8AXga8qeke1fx1MfAsYBVwF/DewYYzO0meBHwaeGtVfXvQ8cyFKY5pqH+rqtpZVauAw+ldaX/OVLv1N6p5Z1TzApgbhslQn2smmBeGQ79ywygVEGPAMya9Phy4c0CxzJmqurPZbgc+Q+9/DKPgW809iBP3Im4fcDxzoqq+1fzjHQc+xBD+Xs19k58G/qqqrmiah/r3muqYRuG3Aqiq+4AvAScBByVZ0rw1EufAvTSSeQHMDcNkFM415oXh03VuGKUC4t+AZzcjzZ8AvAb43IBj2itJDmgG95DkAOAlwM27/9TQ+BxwdvP8bOBvBhjLnJk4mTZeyZD9Xs0ArA8DX6+qP5z01tD+XtMd0zD/VkmWJjmoef5E4Kfp3cP7ReBVzW5D9Tt1ZOTyApgbhs0wn2vAvNDv2PZGP3PDyMzCBNBMt/VHwGJgXVX97oBD2itJltO7sgSwBPj4MB5TksuAU4BDgW8BFwCfBS4HjgD+A3h1VQ3VwLNpjusUel2fBdwBvGHiHtFhkORFwD8DXwXGm+b/i969oUP5e+3mmM5kSH+rJMfSGwi3mN6FoMur6l3NOeMTwI8ANwBnVdXDg4t08EYtL4C5YT4zLwzHbzWKeQH6mxtGqoCQJEmS1K1RuoVJkiRJUscsICRJkiS1ZgEhSZIkqTULCEmSJEmtWUBIkiRJas0CQpoHkpyS5G8HHYckaX4wL2g+s4CQJEmS1JoFhDQDSc5Kcm2STUn+PMniJA8keW+S65P8U5Klzb6rknw5yU1JPpPk4Kb9x5JcleTG5jPPar7+SUk+lWRLkr9qVsqUJM1j5gUtRBYQUktJngP8AvDCqloF7AR+ETgAuL6qXgBsoLfyKMBHgXdU1bH0VrucaP8r4E+r6vnATwATq1weB7wVeC6wHHhh5wclSZo184IWqiWDDkAaIj8FHA/8W3MR6InAdmAc+GSzz8eAK5I8BTioqjY07R8B/jrJgcDTq+ozAFX1EEDzfddW1VjzehOwDPiX7g9LkjRL5gUtSBYQUnsBPlJV5z+mMfmtXfarPXzHdB6e9Hwn/vuUpPnOvKAFyVuYpPb+CXhVkqcCJPmRJM+k9+/oVc0+rwX+paruB/4zyclN+y8BG6rq28BYklc037Fvkv37ehSSpLliXtCCZCUrtVRVX0vyP4EvJFkE/AB4E/BdYGWS64D76d0PC3A28MEmEWwDfrVp/yXgz5O8q/mOV/fxMCRJc8S8oIUqVbvrVZO0J0keqKonDToOSdL8YF7QqPMWJkmSJEmt2QMhSZIkqTV7ICRJkiS1ZgEhSZIkqTULCEmSJEmtWUBIkiRJas0CQpIkSVJr/z9VS80ZWy6RngAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 792x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=1,ncols=2,figsize=(11,5))\n",
    "\n",
    "ax[0].plot(range(1,len(train_loss_list)+1),train_loss_list,'.r', label='train', markersize=8)\n",
    "ax[0].set_ylabel('train loss')\n",
    "ax[0].set_xlabel('epoch')\n",
    "ax[0].legend()\n",
    "ax[1].plot(range(1,len(test_loss_list)+1),test_loss_list, '.b',label='test', markersize=8)\n",
    "ax[1].set_ylabel('test loss')\n",
    "ax[1].set_xlabel('epoch')\n",
    "ax[1].legend()\n",
    "plt.tight_layout()\n",
    "# plt.savefig('VAE_loss.pdf',format='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to save and load model\n",
    "model_path = 'VAE_MNIST.pth'\n",
    "torch.save(model.state_dict(), model_path)\n",
    "model_load=VAE(x_dim=28*28,h1_dim = 500,z_dim=2)\n",
    "model_load.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_img(model,test_loader,n_img):\n",
    "    model.eval()\n",
    "    from torchvision.utils import save_image\n",
    "    with torch.no_grad():\n",
    "        for imgs, label in test_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            save_image(imgs[:n_img], f'original_{list(np.array(label[:n_img]))}.pdf')\n",
    "            x_prob, mu, log_var = model(imgs[:n_img])\n",
    "            save_image(x_prob.view(-1,1,28,28),f'reconstructed_{list(np.array(label[:n_img]))}.pdf')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruct_img(model=model,test_loader=test_loader,n_img=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
